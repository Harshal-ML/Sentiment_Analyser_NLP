# -*- coding: utf-8 -*-
"""MINE natural_language_processing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12NNMqu7hwVmQag7rXxV8LxzU-fqrSGZk

# Natural Language Processing

## Importing the libraries
"""

import numpy as np
import matplotlib.pyplot as plt 
import pandas as pd

"""## Importing the dataset"""

dataset = pd.read_csv("Restaurant_Reviews.tsv", delimiter = "\t", quoting = 3)

"""## Cleaning the texts"""

import re 
import nltk
nltk.download("stopwords")
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
corpus=[]
for i in range (0,1000):
  # 1stly removing all the non alphabets
  review = re.sub("[^a-zA-Z]"," ", dataset.iloc[i,0])
  #review is an atribute of class sub, review variable stores a string
  # 2ndly all words in lower case
  review = review.lower()
  #review is still a string
  # 3rdly spliting each word separately in sentence
  review = review.split()
  # now review is a list with each element a word. required for PorterStemmer
  ps = PorterStemmer()
  all_stopwords = stopwords.words("english")
  #gathering all the stopwords and removing not from this set, because not is negetive review sentiment
  all_stopwords.remove("not")
  all_stopwords.remove("against")
  review= [ps.stem(word) for word in review if word not in set(all_stopwords)]
  #all the stopwords were removed from the list 'review' and stemming was applied
  review = " ".join(review)
  #review variable now again contains a string 
  corpus.append(review)
  #corpus is a list with each review as its elements

print(all_stopwords)

print(corpus)

"""## Creating the Bag of Words model"""

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features = 1550)
#CountVectorizer is used to create the long array of 20000 elements used for creating bag of words
# long array with numbers like 0,1,2 depending on number of times that word occured in our dataset
X = cv.fit_transform(corpus).toarray()
y = dataset.iloc[:,-1].values

print(len(X[0]))

"""## Splitting the dataset into the Training set and Test set"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = 0.2, random_state= 0)

"""## Training the Naive Bayes model on the Training set"""

from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train, y_train)

"""## Predicting the Test set results"""

y_pred = classifier.predict(X_test)
y_pred = y_pred.reshape(len(y_pred), 1)
y_test = y_test.reshape(len(y_test), 1)
np.set_printoptions (precision = 2)
print(np.concatenate((y_test, y_pred),1))

"""## Making the Confusion Matrix"""

from sklearn.metrics import confusion_matrix , accuracy_score
print(confusion_matrix(y_test, y_pred))
print(accuracy_score(y_test, y_pred))